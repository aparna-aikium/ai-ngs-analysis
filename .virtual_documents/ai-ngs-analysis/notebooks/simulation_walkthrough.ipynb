


import math, json, random
from dataclasses import dataclass
from itertools import product
from typing import List, Dict, Tuple, Optional

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from IPython.display import display

try:
    from ipywidgets import (
        VBox, HBox, Accordion, Tab, Layout,
        Dropdown, Textarea, Text, IntText, FloatText, BoundedFloatText,
        Checkbox, Button, HTML, Accordion, IntSlider, FloatSlider, Label
    )
    from IPython.display import display, Markdown, clear_output
    _WIDGETS_OK = True
except Exception:
    _WIDGETS_OK = False
    print("ipywidgets not available; you can still run the core functions below.")





IUPAC_DNA = {
    "A":"A","C":"C","G":"G","T":"T",
    "R":"AG","Y":"CT","S":"GC","W":"AT","K":"GT","M":"AC",
    "B":"CGT","D":"AGT","H":"ACT","V":"ACG","N":"ACGT"
}
DNA4 = list("ACGT")
AA20 = list("ARNDCEQGHILKMFPSTWYV")

def _strip_fasta(seq_text: str) -> str:
    """
    Accepts plain sequence or FASTA text; returns uppercase sequence without headers/spaces.
    """
    lines = [ln.strip() for ln in seq_text.strip().splitlines() if ln.strip()]
    if not lines:
        return ""
    if lines[0].startswith(">"):
        # skip headers
        lines = [ln for ln in lines if not ln.startswith(">")]
    return "".join(lines).upper()

def parse_deg_string(deg: str) -> List[Tuple[int, str]]:
    """
    Parse a string like: "10:N,25:R,42:AC" into [(10, "N"), (25, "R"), (42, "AC")]
    """
    if not deg.strip():
        return []
    out = []
    parts = [p.strip() for p in deg.split(",") if p.strip()]
    for p in parts:
        if ":" not in p:
            raise ValueError(f"Degeneracy item '{p}' must be of the form pos:chars")
        pos_s, chars = p.split(":", 1)
        pos = int(pos_s.strip())
        out.append((pos, chars.strip().upper()))
    return out

def normalize_degeneracy(seq_type: str, chars: str) -> str:
    chars = chars.upper().replace(" ", "")
    if seq_type.lower() == "dna":
        expanded = []
        for ch in chars:
            if ch in IUPAC_DNA:
                expanded.extend(IUPAC_DNA[ch])
            elif ch in "ACGT":
                expanded.append(ch)
            else:
                raise ValueError(f"Invalid DNA char: {ch}")
        return "".join(sorted(set(expanded)))
    # amino acids
    bad = set(chars) - set(AA20)
    if bad:
        raise ValueError(f"Invalid AA chars: {bad}")
    return "".join(sorted(set(chars)))

def theoretical_max(pos_to_choices: Dict[int, str]) -> int:
    m = 1
    for chs in pos_to_choices.values():
        m *= len(chs)
    return m

def generate_library_from_one(
    backbone_seq: str,
    seq_type: str,
    pos_degeneracy: List[Tuple[int, str]],
    n_sequences: Optional[int] = None,
    seed: int = 0
) -> List[str]:
    """
    Generate sequences from a single backbone.
    If n_sequences is None -> enumerate full combinatorial expansion.
    Else: sample unique sequences up to n_sequences (reproducibly by seed).
    """
    L = len(backbone_seq)
    pos_to_choices = {}
    for pos, chars in pos_degeneracy:
        if not (1 <= pos <= L):
            raise ValueError(f"Position {pos} out of 1..{L}")
        pos_to_choices[pos] = normalize_degeneracy(seq_type, chars)

    tmax = theoretical_max(pos_to_choices)
    rng = random.Random(seed)

    if n_sequences is None:
        # full enumeration
        if not pos_to_choices:
            return [backbone_seq]
        positions = sorted(pos_to_choices.keys())
        choice_lists = [list(pos_to_choices[p]) for p in positions]
        seqs = []
        for combo in product(*choice_lists):
            s = list(backbone_seq)
            for p, ch in zip(positions, combo):
                s[p-1] = ch
            seqs.append("".join(s))
        return seqs

    # sample unique
    n = min(int(n_sequences), tmax if tmax > 0 else int(n_sequences))
    seqs_set = set()
    def mutate():
        s = list(backbone_seq)
        for p, choices in pos_to_choices.items():
            s[p-1] = rng.choice(choices)
        return "".join(s)
    while len(seqs_set) < n:
        seqs_set.add(mutate())
    return list(seqs_set)

def selection_simulate_rounds(
    df_library: pd.DataFrame,
    rounds: int = 3,
    display: str = "yeast",
    mode: str = "competitive",
    target_conc_nM: float = 50.0,
    S: float = 0.7,
    pcr_cycles: int = 10,
    bias_sigma: float = 0.3,
    expr_enabled: bool = True,
    expr_sigma: float = 0.5,
    KD_mu_log10: float = -7.0,
    KD_alpha: float = 1.0,
    KD_sigma: float = 0.3,
    input_molecules: int = 200_000,
    seed: int = 4242
):
    """
    Minimal display-selection simulator (single target for clarity).
    Returns list of per-round DataFrames.
    """
    rng = np.random.default_rng(seed)
    N = len(df_library)
    # start frequencies
    if "start_frequency" not in df_library.columns:
        freq = np.ones(N) / N
    else:
        freq = df_library["start_frequency"].to_numpy()
        freq = freq / freq.sum()

    # expression heterogeneity (yeast-like)
    expr = rng.lognormal(mean=0.0, sigma=expr_sigma, size=N) if expr_enabled else np.ones(N)

    # per-sequence baseline "trait"
    a = rng.normal(0, 1.0, size=N)
    # KD on log10 scale -> numeric KD (relative units)
    log10KD = KD_mu_log10 - KD_alpha * a + rng.normal(0, KD_sigma, size=N)
    KD = 10.0 ** log10KD

    def occupancy(C, KD):
        return C / (C + KD)

    rounds_out = []
    for r in range(1, rounds+1):
        # capture probability ~ S * occupancy * expression
        p_bind = S * occupancy(target_conc_nM, KD) * expr
        eff = p_bind * freq
        if eff.sum() <= 0:
            eff = np.ones_like(eff) / len(eff)
        probs = eff / eff.sum()

        # captured counts (multinomial)
        captured = rng.multinomial(input_molecules, probs)

        # PCR amplification with bias
        G = 2 ** pcr_cycles
        bias = rng.lognormal(mean=0.0, sigma=bias_sigma, size=N)
        lam = captured * G * bias
        amplified = rng.poisson(lam)
        total = max(amplified.sum(), 1)
        freq = amplified / total

        out = df_library.copy()
        out["round"] = r
        out["capture_prob"] = p_bind
        out["captured"] = captured
        out["amplified"] = amplified
        out["frequency"] = freq
        out["KD"] = KD
        rounds_out.append(out)

    return rounds_out

def ngs_from_pool(df_pool: pd.DataFrame, reads_total: int, p_error: float, seed: int = 999):
    """
    Simulate NGS reads from a pool with 'sequence' and 'frequency' columns.
    Returns a DataFrame of reads with error annotations.
    """
    if "frequency" not in df_pool.columns:
        raise ValueError("df_pool must have a 'frequency' column")
    seqs = df_pool["sequence"].astype(str).str.upper().to_numpy()
    probs = df_pool["frequency"].to_numpy()
    probs = probs / probs.sum()
    rng = np.random.default_rng(seed)

    # choose DNA/AA alphabet based on first seq
    is_dna = set(seqs[0]).issubset(set("ACGTN"))
    alpha = list("ACGT") if is_dna else AA20

    # CDF sample
    cdf = np.cumsum(probs)
    def pick_idx(u):
        return np.searchsorted(cdf, u, side="right")

    reads = []
    for i in range(reads_total):
        idx = pick_idx(rng.random())
        src = seqs[idx]
        obs_chars, errs = [], 0
        for ch in src:
            if rng.random() < p_error:
                # flip to a different character in alphabet
                choices = [a for a in alpha if a != ch]
                obs_chars.append(choices[int(rng.integers(0, len(choices)))])
                errs += 1
            else:
                obs_chars.append(ch)
        reads.append({"read_id": f"read_{i:07d}", "source_seq": src, "observed_seq": "".join(obs_chars), "num_errors": errs})
    return pd.DataFrame(reads)





LIB_DF = None
ROUNDS = None
READS_DF = None


# Inline backend + crisp figs
%matplotlib inline
import matplotlib
matplotlib.rcParams["figure.dpi"] = 120

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from IPython.display import display, clear_output, Markdown, HTML
import ipywidgets as W

# helper: pretty DataFrame HTML (no truncation, wraps long seqs)
def render_df(df, max_rows=10):
    html = (
        df.head(max_rows)
          .to_html(index=False, escape=False)
          .replace('<table', '<table style="font-size:12px;border-collapse:collapse;table-layout:fixed;width:100%"')
          .replace('<th>', '<th style="text-align:left;border-bottom:1px solid #ddd;padding:4px 8px;white-space:normal">')
          .replace('<td>', '<td style="text-align:left;padding:4px 8px;vertical-align:top;word-break:break-word;white-space:normal">')
    )
    return f'<div style="max-height:340px;overflow:auto">{html}</div>'

# tiny ⓘ icon with hover text
def info_icon(text):
    return W.HTML(value=f"<span title='{text}' style='cursor:help;font-weight:bold'>ⓘ</span>")

# ---- OUTPUT AREAS (plots/text land here) ----
out_gen_txt   = W.HTML()
out_gen_fig   = W.Output()
out_sel_txt   = W.HTML()
out_sel_fig   = W.Output()
out_ngs_txt   = W.HTML()
out_ngs_fig   = W.Output()

# ---- WIDGETS (wide + non-truncated labels) ----
wide = {'width':'760px'}
label_style = {'description_width':'initial'}

# Library
dd_mode   = W.Dropdown(options=[("Single reference","single"), ("Multiple references","multi")],
                       value="single", description="Mode:", layout=W.Layout(width='260px'),
                       style=label_style, tooltip="Choose 1 or 2 reference backbones")
ta_seqA   = W.Textarea(
    value=">A\nATGCGTACGTGCTAGCTAGCTGATCGATGCTAGCTAGGCTAGCTAGCTAGGATCCGATCGTAGCTAGCTAGCTAGCTA",
    description="Backbone A:", layout=W.Layout(**wide, height='80px'), style=label_style,
    tooltip="FASTA or raw sequence")
txt_degA  = W.Text(value="10:N,25:R,42:AC", description="Deg A (pos:chars):",
                   layout=W.Layout(width='380px'), style=label_style,
                   tooltip="Comma list like 10:N,25:R (1-based positions)")
int_nA    = W.IntText(value=500, description="Sample N(A):", layout=W.Layout(width='180px'), style=label_style)
float_wA  = W.FloatText(value=1.0, description="Weight A:", layout=W.Layout(width='180px'), style=label_style)

ta_seqB   = W.Textarea(
    value=">B\nATGCGTACGATCGTAGCTAGCTAGGCTAGCTAGGATCCGATCGTAGCTAGCTAGCTAGCTAGCGTACGTAGCTAGCTA",
    description="Backbone B:", layout=W.Layout(**wide, height='80px'), style=label_style)
txt_degB  = W.Text(value="8:AC,40:N", description="Deg B (pos:chars):",
                   layout=W.Layout(width='380px'), style=label_style)
int_nB    = W.IntText(value=500, description="Sample N(B):", layout=W.Layout(width='180px'), style=label_style)
float_wB  = W.FloatText(value=0.5, description="Weight B:", layout=W.Layout(width='180px'), style=label_style)

cb_full      = W.Checkbox(value=False, description="Enumerate full combinatorial (ignore N samples)",
                          style=label_style, tooltip="If checked, enumerate all combinations")
int_seed_lib = W.IntText(value=12345, description="Seed (library):", layout=W.Layout(width='220px'), style=label_style)
btn_gen      = W.Button(description="Generate Library", button_style="success", layout=W.Layout(width='200px'))

# Selection (now includes visualization)
cb_enable_sel = W.Checkbox(value=True, description="Enable selection", style=label_style)

int_rounds    = W.IntSlider(value=3, min=1, max=6, step=1, description="Rounds", style=label_style)
float_S       = W.BoundedFloatText(value=0.7, min=0, max=1, step=0.05, description="Stringency S", style=label_style,
                                   tooltip="Scales capture probability (higher → stricter)")
float_C       = W.BoundedFloatText(value=50.0, min=0.01, max=1e6, step=1.0, description="Target C (nM)", style=label_style)
cb_expr       = W.Checkbox(value=True, description="Expression heterogeneity", style=label_style,
                           tooltip="Cell-to-cell variation; favors high expressers when >0")
float_expr    = W.BoundedFloatText(value=0.5, min=0, max=2.0, step=0.05, description="Expr σ", style=label_style)
int_cycles    = W.IntSlider(value=10, min=0, max=20, step=1, description="PCR cycles", style=label_style)
float_bias    = W.BoundedFloatText(value=0.3, min=0, max=2.0, step=0.05, description="PCR bias σ", style=label_style)
float_muKD    = W.FloatText(value=-7.0, description="μ(log10 KD)", style=label_style)
float_alphaKD = W.FloatText(value=1.0, description="α (affinity spread)", style=label_style)
float_sigmaKD = W.FloatText(value=0.3, description="σ KD", style=label_style)
int_input     = W.IntText(value=200000, description="Molecules/round", style=label_style)
int_seed_sel  = W.IntText(value=4242, description="Seed (select)", style=label_style)

# NEW: Abundance visualization integrated into selection
dd_abund  = W.Dropdown(options=[("Uniform","uniform"), ("Log-normal (σ=1.0)","ln1"), ("Log-normal (σ=0.5)","ln0.5")],
                       value="uniform", description="Abundance model", style=label_style)
btn_sel       = W.Button(description="Run Selection + Show Abundance", button_style="info")

# NGS
cb_enable_ngs = W.Checkbox(value=True, description="Enable NGS", style=label_style)
dd_ngs_source  = W.Dropdown(options=[("From selection round","selection"), ("From true library (uniform)","library")],
                            value="selection", description="NGS source", style=label_style)
int_ngs_round  = W.IntSlider(value=3, min=1, max=6, step=1, description="Use round", style=label_style)
int_reads      = W.IntText(value=10000, description="Reads total", style=label_style)
float_perr     = W.BoundedFloatText(value=0.001, min=0, max=0.2, step=0.0005, description="Per‑base error p", style=label_style)
int_seed_ngs   = W.IntText(value=999, description="Seed (NGS)", style=label_style)
btn_ngs        = W.Button(description="Simulate NGS", button_style="warning")

# ---- Tooltips for labels (fix 'null' hovers) ----
def _set_tip(widget, text):
    # v8+ uses .tooltip; v7 used .description_tooltip
    if hasattr(widget, "tooltip"):
        widget.tooltip = text
    else:  # fallback for very old ipywidgets
        try:
            widget.description_tooltip = text
        except Exception:
            pass             # fallback

def attach_tooltips():
    # Library
    _set_tip(dd_mode, "Use one or two reference backbones.")
    _set_tip(ta_seqA, "Reference sequence A (FASTA or raw; '>' header lines ignored).")
    _set_tip(txt_degA, "Degeneracy for A: comma list of 1‑based pos:chars. Example 10:N,25:R,42:AC")
    _set_tip(int_nA, "If full enumeration is OFF, sample this many unique A variants.")
    _set_tip(float_wA, "Relative abundance weight for backbone A when mixing A and B.")
    _set_tip(ta_seqB, "Reference sequence B (optional if Mode=Single).")
    _set_tip(txt_degB, "Degeneracy for B: comma list of 1‑based pos:chars. Example 8:AC,40:N")
    _set_tip(int_nB, "If full enumeration is OFF, sample this many unique B variants.")
    _set_tip(float_wB, "Relative abundance weight for backbone B when mixing A and B.")
    _set_tip(cb_full, "If checked, enumerate ALL combinations from degeneracy and ignore Sample N(A/B).")
    _set_tip(int_seed_lib, "Random seed for reproducible library sampling.")
    _set_tip(btn_gen, "Generate the ground‑truth library and show preview + length histogram.")

    # Selection
    _set_tip(cb_enable_sel, "Toggle the selection simulator on/off.")
    _set_tip(int_rounds, "Number of selection cycles (bind→wash→PCR).")
    _set_tip(float_S, "Stringency S (0–1). Probability cutoff for survival. Higher = harsher filtering (tight binders survive).")
    _set_tip(float_C, "Target (ligand) concentration in nM. Higher = easier for weak binders; lower favors only strong binders).")
    _set_tip(cb_expr, "Include cell‑to‑cell expression heterogeneity.")
    _set_tip(float_expr, "Expression noise σ (0=uniform; higher → some clones over/under-express).")
    _set_tip(int_cycles, "PCR cycles per round (amplification depth).")
    _set_tip(float_bias, "PCR bias σ per round (skew / jackpotting).")
    _set_tip(float_muKD, "Mean log10 KD across variants (e.g., −7 ≈ 100 nM).")
    _set_tip(float_alphaKD, "Affinity spread parameter α (larger → broader KD spectrum).")
    _set_tip(float_sigmaKD, "Per‑variant KD noise σ (measurement/realization variability).")
    _set_tip(int_input, "Total molecules sampled per round (bottleneck).")
    _set_tip(int_seed_sel, "Random seed for the selection simulation.")
    _set_tip(dd_abund, "Choose abundance model for visualization: uniform (even) vs log-normal (skewed).")
    _set_tip(btn_sel, "Run selection rounds and show abundance skew plots.")

    # NGS
    _set_tip(cb_enable_ngs, "Toggle NGS read simulation on/off.")
    _set_tip(dd_ngs_source, "Read from a selection round (skewed) vs true library (uniform).")
    _set_tip(int_ngs_round, "Which selection round to use as the NGS source.")
    _set_tip(int_reads, "Total number of reads to simulate.")
    _set_tip(float_perr, "Per‑base substitution probability (e.g., 0.001 ≈ Q30).")
    _set_tip(int_seed_ngs, "Random seed for the NGS simulation.")
    _set_tip(btn_ngs, "Simulate reads and plot error distribution + coverage.")

# Help lines (hover ⓘ and short blurbs)
help_lib = W.HTML(
    "Backbone: reference sequence. Deg: <code>pos:chars</code> (1‑based). "
    "Sample N: how many variants to draw (unchecked = enumerate all). Weight: mix ratio when combining A/B."
)
help_sel = W.HTML(
    "S: stricter selection ↑. Target C: higher makes binding easier. "
    "Expr σ: expression variability. PCR cycles + bias σ: amplification & skew. "
    "KD params (μ, α, σ) shape affinity. Abundance model: visualize skew patterns."
)
help_ngs = W.HTML(
    "NGS source: selected pool (skewed) vs true library (uniform). Per‑base error p: substitution chance per position. "
    "Shows error distribution and coverage plots."
)

# Data holders
LIB_DF, ROUNDS, READS_DF = None, None, None

# ---- LAYOUT / RENDER ----
lib_box = W.VBox([
    W.HBox([dd_mode, info_icon("Use one or two reference backbones.")]),
    ta_seqA, W.HBox([txt_degA, int_nA, float_wA]),
    ta_seqB, W.HBox([txt_degB, int_nB, float_wB]),
    W.HBox([cb_full, int_seed_lib, btn_gen]),
    help_lib, out_gen_txt, out_gen_fig
])

# UPDATED: Selection box now includes abundance visualization
sel_box = W.VBox([
    W.HBox([cb_enable_sel, int_rounds, float_S, float_C]),
    W.HBox([cb_expr, float_expr, int_cycles, float_bias]),
    W.HBox([float_muKD, float_alphaKD, float_sigmaKD, int_input, int_seed_sel]),
    W.HBox([dd_abund, btn_sel]),  # Abundance model and selection button
    help_sel, out_sel_txt, out_sel_fig
])

ngs_box = W.VBox([
    W.HBox([cb_enable_ngs, dd_ngs_source, int_ngs_round]),
    W.HBox([int_reads, float_perr, int_seed_ngs, btn_ngs]),
    help_ngs, out_ngs_txt, out_ngs_fig
])

# attach tooltip text to labels BEFORE rendering
attach_tooltips()

display(Markdown("### 1) Library generation")); display(lib_box)
display(Markdown("### 2) Selection + Abundance visualization"));  display(sel_box)  # UPDATED
display(Markdown("### 3) NGS simulation"));       display(ngs_box)

# ---- CALLBACKS (write to Output panes) ----
def on_generate(_):
    global LIB_DF, ROUNDS
    ROUNDS = None
    with out_gen_fig:
        clear_output(wait=True)
        try:
            mode = dd_mode.value
            seqA = _strip_fasta(ta_seqA.value)
            degA = parse_deg_string(txt_degA.value)
            nA   = None if cb_full.value else int_nA.value
            wA   = float_wA.value

            recs = []
            seqsA = generate_library_from_one(seqA, "dna", degA, n_sequences=nA, seed=int_seed_lib.value)
            for i, s in enumerate(seqsA):
                recs.append({"seq_id": f"A_{i:05d}", "sequence": s, "backbone_id":"A", "start_weight": wA})

            if mode == "multi":
                seqB = _strip_fasta(ta_seqB.value)
                degB = parse_deg_string(txt_degB.value)
                nB   = None if cb_full.value else int_nB.value
                wB   = float_wB.value
                seqsB = generate_library_from_one(seqB, "dna", degB, n_sequences=nB, seed=int_seed_lib.value+101)
                for i, s in enumerate(seqsB):
                    recs.append({"seq_id": f"B_{i:05d}", "sequence": s, "backbone_id":"B", "start_weight": wB})

            df = pd.DataFrame(recs)
            df["start_frequency"] = df["start_weight"] / df["start_weight"].sum()
            LIB_DF = df

            out_gen_txt.value = f"<b>✅ Generated library with {len(df)} sequences.</b>" + render_df(
                df[["seq_id","sequence","backbone_id","start_weight","start_frequency"]]
            )

            fig, ax = plt.subplots(figsize=(6,3))
            ax.hist(df["sequence"].str.len(), bins=10)
            ax.set_title("Sequence length distribution")
            ax.set_xlabel("Length (nt)"); ax.set_ylabel("Count")
            display(fig); plt.close(fig)

        except Exception as e:
            out_gen_txt.value = f"<pre style='color:red'>Error: {e}</pre>"

def on_selection(_):
    global ROUNDS
    with out_sel_fig:
        clear_output(wait=True)
        if not cb_enable_sel.value:
            out_sel_txt.value = "<pre>Selection disabled — skipping.</pre>"; ROUNDS=None; return
        if LIB_DF is None or LIB_DF.empty:
            out_sel_txt.value = "<pre style='color:red'>Generate the library first.</pre>"; return
        try:
            rounds       = int_rounds.value
            S            = float_S.value
            C            = float_C.value
            cycles       = int_cycles.value
            bias_sig     = float_bias.value
            expr         = cb_expr.value
            expr_sig     = float_expr.value
            muKD, aKD, sKD = float_muKD.value, float_alphaKD.value, float_sigmaKD.value
            mols         = int_input.value
            seed         = int_seed_sel.value

            ROUNDS = selection_simulate_rounds(
                LIB_DF, rounds=rounds, display="generic", S=S, target_conc_nM=C,
                pcr_cycles=cycles, bias_sigma=bias_sig, expr_enabled=expr, expr_sigma=expr_sig,
                KD_mu_log10=muKD, KD_alpha=aKD, KD_sigma=sKD, input_molecules=mols, seed=seed
            )
            final = ROUNDS[-1]
            top5  = final.sort_values("frequency", ascending=False).head(5)
            out_sel_txt.value = f"<b>✅ Ran {rounds} round(s). Final pool size: {len(final)}.</b>" + render_df(
                top5[["seq_id","sequence","backbone_id","round","frequency","KD","capture_prob"]]
            )

            # Plots
            sh = []
            for rdf in ROUNDS:
                f = rdf["frequency"].to_numpy(); f = f / f.sum()
                sh.append(-(f * np.where(f>0, np.log(f), 0)).sum())
            figH, axH = plt.subplots(figsize=(6,3))
            axH.plot(range(1,len(ROUNDS)+1), sh, marker="o")
            axH.set_title("Diversity (Shannon) across rounds")
            axH.set_xlabel("Round"); axH.set_ylabel("Shannon H")
            display(figH); plt.close(figH)

            figT, axT = plt.subplots(figsize=(7,4))
            top_ids = final.sort_values("frequency", ascending=False).head(10)["seq_id"].tolist()
            for rid in top_ids:
                ys = []
                for rdf in ROUNDS:
                    row = rdf.loc[rdf["seq_id"] == rid]
                    ys.append(row["frequency"].values[0] if not row.empty else 0.0)
                axT.plot(range(1,len(ROUNDS)+1), ys, marker="o")
            axT.set_title("Top‑10 frequency trajectories")
            axT.set_xlabel("Round"); axH.set_ylabel("Frequency")
            display(figT); plt.close(figT)

            r1 = ROUNDS[0]
            figKD, axKD = plt.subplots(figsize=(6,3))
            axKD.hist(r1["KD"], bins=40); axKD.set_xscale("log")
            axKD.set_title("Round 1 affinity (KD)"); axKD.set_xlabel("KD"); axKD.set_ylabel("Count")
            display(figKD); plt.close(figKD)

            figCap, axCap = plt.subplots(figsize=(6,3))
            axCap.hist(r1["capture_prob"], bins=40)
            axCap.set_title("Round 1 capture probability"); axCap.set_xlabel("p_capture"); axCap.set_ylabel("Count")
            display(figCap); plt.close(figCap)

            # NEW: Abundance visualization integrated into selection
            model = dd_abund.value
            n = 5000
            rng = np.random.default_rng(123)
            if model == "uniform":
                weights = np.ones(n)
            elif model == "ln1":
                weights = rng.lognormal(mean=0.0, sigma=1.0, size=n)
            else:
                weights = rng.lognormal(mean=0.0, sigma=0.5, size=n)
            weights = weights / weights.sum()

            fig1, ax1 = plt.subplots(figsize=(6,3))
            ax1.hist(weights, bins=50)
            ax1.set_title(f"Abundance weights: {model}")
            ax1.set_xlabel("Weight"); ax1.set_ylabel("Count")
            display(fig1); plt.close(fig1)

            w_sorted = np.sort(weights)[::-1]
            topk = np.arange(1,51); cum = np.cumsum(w_sorted[:50])
            fig2, ax2 = plt.subplots(figsize=(6,3))
            ax2.plot(topk, cum, marker="o")
            ax2.set_title("Cumulative weight of top‑K sequences")
            ax2.set_xlabel("Top‑K"); ax2.set_ylabel("Cumulative fraction")
            display(fig2); plt.close(fig2)

            out_sel_txt.value += "<br><br><b>Abundance Skew Visualization:</b><br>"
            out_sel_txt.value += "Tip: log‑normal concentrates mass into few high‑abundance variants; uniform spreads it evenly."

        except Exception as e:
            out_sel_txt.value = f"<pre style='color:red'>Error: {e}</pre>"

def on_ngs(_):
    global READS_DF
    with out_ngs_fig:
        clear_output(wait=True)
        if not cb_enable_ngs.value:
            out_ngs_txt.value = "<pre>NGS disabled — skipping.</pre>"; READS_DF=None; return
        if LIB_DF is None or LIB_DF.empty:
            out_ngs_txt.value = "<pre style='color:red'>Generate the library first.</pre>"; return
        try:
            src         = dd_ngs_source.value
            reads_total = int_reads.value
            p_err       = float_perr.value
            seed        = int_seed_ngs.value

            if src == "selection":
                if not ROUNDS:
                    out_ngs_txt.value = "<pre style='color:red'>Run selection first (or change source).</pre>"; return
                idx  = max(0, min(int_ngs_round.value-1, len(ROUNDS)-1))
                pool = ROUNDS[idx][["seq_id","sequence","frequency"]].copy()
                READS_DF = ngs_from_pool(pool, reads_total, p_err, seed=seed)
                src_label = f"selection round {idx+1}"
            else:
                pool = LIB_DF.copy(); pool["frequency"] = 1.0/len(pool)
                READS_DF = ngs_from_pool(pool[["sequence","frequency"]], reads_total, p_err, seed=seed)
                src_label = "true library (uniform)"

            out_ngs_txt.value = f"<b>✅ Simulated {len(READS_DF)} reads from {src_label}.</b>" + render_df(READS_DF)

            # NGS-specific plots: error distribution and coverage
            figE, axE = plt.subplots(figsize=(6,3))
            axE.hist(READS_DF["num_errors"], bins=20)
            axE.set_title("NGS per‑read error count")
            axE.set_xlabel("# errors per read"); axE.set_ylabel("Reads")
            display(figE); plt.close(figE)

            # NEW: Coverage plot
            if src == "selection":
                # Show how reads cover the selected sequences
                coverage_data = READS_DF.groupby("source_seq").size().sort_values(ascending=False)
                figC, axC = plt.subplots(figsize=(6,3))
                axC.bar(range(len(coverage_data)), coverage_data.values)
                axC.set_title("Read coverage per sequence (selection source)")
                axC.set_xlabel("Sequence rank"); axC.set_ylabel("Read count")
                axC.set_yscale("log")
                display(figC); plt.close(figC)

                # Show frequency vs coverage correlation
                if len(pool) > 1:
                    pool_with_coverage = pool.copy()
                    pool_with_coverage["read_count"] = pool_with_coverage["sequence"].map(
                        READS_DF.groupby("source_seq").size()
                    ).fillna(0)
                    
                    figFC, axFC = plt.subplots(figsize=(6,3))
                    axFC.scatter(pool_with_coverage["frequency"], pool_with_coverage["read_count"], alpha=0.6)
                    axFC.set_xlabel("Selection frequency"); axFC.set_ylabel("Read count")
                    axFC.set_title("Frequency vs Read Coverage")
                    axFC.set_yscale("log")
                    display(figFC); plt.close(figFC)

        except Exception as e:
            out_ngs_txt.value = f"<pre style='color:red'>Error: {e}</pre>"

# wire up
btn_gen.on_click(on_generate)
btn_sel.on_click(on_selection)
btn_ngs.on_click(on_ngs)








# ---- BOWTIE2 ALIGNMENT SETUP ----
print("Bowtie2 Alignment Setup")
print("Make sure you've run 'Generate Library' and 'Run NGS' first!")

# Bowtie2 alignment Imports
import subprocess
import shlex
from pathlib import Path
import json
from datetime import datetime
import glob
import shutil
import time

# Check if required tools are available
def check_tools():
    tools = ['bowtie2', 'bowtie2-build', 'samtools']
    missing = []
    for tool in tools:
        try:
            subprocess.run([tool, '--version'], capture_output=True, check=True)
        except (subprocess.CalledProcessError, FileNotFoundError):
            missing.append(tool)
    
    if missing:
        print(f"Missing tools: {', '.join(missing)}")
        print("Please install: conda install -c bioconda bowtie2 samtools")
        return False
    else:
        print("All required tools available")
        return True

# Discover library references
def discover_library_references():
    references = []
    
    # Priority 1: Current session library
    if 'LIB_DF' in globals() and LIB_DF is not None and not LIB_DF.empty:
        temp_fasta = "temp_current_library.fasta"
        with open(temp_fasta, 'w') as f:
            for _, row in LIB_DF.iterrows():
                f.write(f">{row['seq_id']}\n{row['sequence']}\n")
        references.append({
            'name': f"Current Session Library ({len(LIB_DF)} seqs, current_session)",
            'fasta': temp_fasta,
            'type': 'current_session'
        })
        print(f"Found current session library with {len(LIB_DF)} sequences.")
    
    # Priority 2: Saved libraries
    library_paths = [
        Path("data/libraries"),
        Path("../data/libraries"),
        Path("../../data/libraries")
    ]
    
    for lib_path in library_paths:
        if lib_path.exists():
            for fasta_file in lib_path.glob("*.fasta"):
                references.append({
                    'name': f"{fasta_file.stem} ({fasta_file.parent.name})",
                    'fasta': str(fasta_file),
                    'type': 'saved'
                })
    
    print(f"Found {len(references)} libraries.")
    return references

# Discover simulated reads - FIXED with correct column names
def discover_simulated_reads():
    reads = []
    
    # Priority 1: Current session reads
    if 'READS_DF' in globals() and READS_DF is not None and not READS_DF.empty:
        temp_fasta = "temp_current_reads.fasta"
        
        with open(temp_fasta, 'w') as f:
            for _, row in READS_DF.iterrows():
                # Use observed_seq (the simulated reads with errors)
                seq = row['observed_seq']
                read_id = row['read_id']
                f.write(f">{read_id}\n{seq}\n")
        
        reads.append({
            'name': f"Current Session Reads ({len(READS_DF)} seqs, current_session)",
            'fasta': temp_fasta,
            'type': 'current_session'
        })
        print(f"Found current session reads with {len(READS_DF)} sequences.")
    
    # Priority 2: Saved read sets
    read_paths = [
        Path("data/reads"),
        Path("../data/reads"),
        Path("../../data/reads")
    ]
    
    for read_path in read_paths:
        if read_path.exists():
            for fasta_file in read_path.glob("*.fasta"):
                reads.append({
                    'name': f"{fasta_file.stem} ({fasta_file.parent.name})",
                    'fasta': str(fasta_file),
                    'type': 'saved'
                })
    
    print(f"Found {len(reads)} read sets.")
    return reads

# Check tools but don't run discovery yet
if check_tools():
    print("Tools available - discovery will happen when you refresh the interface")
else:
    print("Some tools missing - please install required packages")





# ---- BOWTIE2 ALIGNMENT INTERFACE ----
# Create Bowtie2 widgets - MUCH WIDER to prevent truncation
w_ref_selection = W.Dropdown(
    options=[],
    description="",
    layout=W.Layout(width='600px'),  # Much wider
    style={'description_width': 'initial'}
)

w_reads_selection = W.Dropdown(
    options=[],
    description="",
    layout=W.Layout(width='600px'),  # Much wider
    style={'description_width': 'initial'}
)

w_outdir = W.Text(
    value=f"data/alignments/{datetime.now().strftime('%Y%m%d-%H%M%S')}_notebook_alignment",
    description="",
    layout=W.Layout(width='600px'),  # Much wider
    style={'description_width': 'initial'}
)

w_preset = W.Dropdown(
    options=["--very-fast", "--fast", "--sensitive", "--very-sensitive", "--very-sensitive-local"],
    value="--very-sensitive",
    description="",
    layout=W.Layout(width='300px'),  # Wider
    style={'description_width': 'initial'}
)

# Parameter widgets - MUCH WIDER
w_N = W.IntText(value=1, description="", layout=W.Layout(width='200px'))  # Wider
w_L = W.IntText(value=20, description="", layout=W.Layout(width='200px'))  # Wider
w_i = W.Text(value="S,1,0.75", description="", layout=W.Layout(width='200px'))  # Wider
w_score_min = W.Text(value="L,-0.6,-0.6", description="", layout=W.Layout(width='200px'))  # Wider
w_threads = W.IntText(value=4, description="", layout=W.Layout(width='200px'))  # Wider
w_mapq_min = W.IntText(value=0, description="", layout=W.Layout(width='200px'))  # Wider

# Buttons - MUCH WIDER
w_run_bowtie2 = W.Button(description="Run Bowtie2 Alignment", button_style='success', layout=W.Layout(width='250px'))
w_run_analysis = W.Button(description="Run Quick Analysis", button_style='info', layout=W.Layout(width='250px'))
w_refresh_discovery = W.Button(description="Refresh Discovery", button_style='primary', layout=W.Layout(width='200px'))

w_bowtie2_output = W.Output()
w_analysis_output = W.Output()

# Function to refresh discovery and update dropdowns
def refresh_discovery_and_dropdowns():
    """Refresh discovery and update all dropdowns"""
    with w_bowtie2_output:
        clear_output(wait=True)
        print("Refreshing discovery...")
        
        # Discover libraries and reads
        available_references = discover_library_references()
        available_reads = discover_simulated_reads()
        
        # Update reference dropdown
        ref_options = [(ref['name'], ref) for ref in available_references]
        w_ref_selection.options = ref_options
        
        # Update reads dropdown  
        read_options = [(read['name'], read) for read in available_reads]
        w_reads_selection.options = read_options
        
        print(f"Updated dropdowns: {len(available_references)} references, {len(available_reads)} read sets")
        
        if not available_references:
            print("No references found. Make sure you've generated a library first.")
        if not available_reads:
            print("No reads found. Make sure you've run NGS simulation first.")

# Bowtie2 alignment function - FIXED to reduce verbose output
def run_bowtie2_alignment(ref_data, reads_data, outdir, preset, N, L, i, score_min, threads, mapq_min):
    """Run Bowtie2 alignment with given parameters"""
    try:
        # Create output directory
        Path(outdir).mkdir(parents=True, exist_ok=True)
        
        # Handle reference data (could be file path or DataFrame)
        if isinstance(ref_data, dict) and 'fasta' in ref_data:
            ref_fasta = ref_data['fasta']
        elif isinstance(ref_data, str):
            ref_fasta = ref_data
        else:
            raise ValueError("Invalid reference data format")
        
        # Handle reads data (could be file path or DataFrame)
        if isinstance(reads_data, dict) and 'fasta' in reads_data:
            reads_fasta = reads_data['fasta']
        elif isinstance(reads_data, str):
            reads_fasta = reads_data
        else:
            raise ValueError("Invalid reads data format")
        
        # Build Bowtie2 index - SUPPRESS VERBOSE OUTPUT
        index_base = Path(outdir) / "reference_index"
        print("Building Bowtie2 index...")
        
        build_cmd = [
            'bowtie2-build',
            '--quiet',  # Suppress verbose output
            ref_fasta,
            str(index_base)
        ]
        
        # Run build command and only show errors if they occur
        result = subprocess.run(build_cmd, capture_output=True, text=True, check=True)
        
        # Only show output if there are warnings/errors
        if result.stderr and result.stderr.strip():
            print("Build warnings/errors:")
            print(result.stderr)
        
        print("Index built successfully")
        
        # Verify index files exist
        index_files = list(index_base.parent.glob(f"{index_base.name}.*"))
        print(f"Index files created: {len(index_files)} files")
        
        # Small delay to ensure files are flushed
        time.sleep(1)
        
        # Run Bowtie2 alignment
        sam_output = Path(outdir) / "alignment.sam"
        bam_output = Path(outdir) / "alignment.bam"
        sorted_bam = Path(outdir) / "alignment.sorted.bam"
        
        print("Running Bowtie2 alignment...")
        
        # FIXED: Added -f flag to specify FASTA format for reads
        bowtie2_cmd = [
            'bowtie2',
            preset,
            f'-N', str(N),
            f'-L', str(L),
            f'-i', i,
            f'--score-min', score_min,
            f'-p', str(threads),
            f'-f',  # Specify that reads are in FASTA format
            f'-x', str(index_base),
            f'-U', reads_fasta,
            f'-S', str(sam_output)
        ]
        
        # Run alignment with minimal output
        result = subprocess.run(bowtie2_cmd, capture_output=True, text=True, check=True)
        
        # Only show alignment output if there are warnings/errors
        if result.stderr and result.stderr.strip():
            print("Alignment warnings/errors:")
            print(result.stderr)
        
        print("Alignment completed successfully")
        
        # Convert SAM to BAM, sort, and index
        print("Converting SAM to BAM...")
        subprocess.run(['samtools', 'view', '-b', str(sam_output)], stdout=open(bam_output, 'wb'), check=True)
        
        print("Sorting BAM...")
        subprocess.run(['samtools', 'sort', str(bam_output), '-o', str(sorted_bam)], check=True)
        
        print("Indexing BAM...")
        subprocess.run(['samtools', 'index', str(sorted_bam)], check=True)
        
        # Clean up intermediate files
        sam_output.unlink(missing_ok=True)
        bam_output.unlink(missing_ok=True)
        
        print(f"Alignment complete! Results saved to {outdir}")
        return str(sorted_bam)
        
    except subprocess.CalledProcessError as e:
        print(f"Error running Bowtie2: {e}")
        print(f"STDOUT: {e.stdout}")
        print(f"STDERR: {e.stderr}")
        raise
    except Exception as e:
        print(f"Unexpected error: {e}")
        raise

# Quick BAM analysis
def quick_bam_analysis(bam_path):
    """Perform quick analysis of BAM file"""
    try:
        print("=== BAM Analysis ===")
        
        # Flagstat
        print("\n--- Flagstat ---")
        result = subprocess.run(['samtools', 'flagstat', bam_path], capture_output=True, text=True, check=True)
        print(result.stdout)
        
        # Idxstats
        print("\n--- Index Stats ---")
        result = subprocess.run(['samtools', 'idxstats', bam_path], capture_output=True, text=True, check=True)
        print(result.stdout)
        
        # Sample alignments
        print("\n--- Sample Alignments ---")
        result = subprocess.run(['samtools', 'view', bam_path, '-h'], capture_output=True, text=True, check=True)
        lines = result.stdout.split('\n')
        for i, line in enumerate(lines):
            if line.startswith('@'):
                print(line)
            elif line and i < 10:  # Show first 10 alignments
                print(line)
            if i >= 20:  # Limit output
                break
        
    except Exception as e:
        print(f"Error analyzing BAM: {e}")

# Get recent BAM files
def get_recent_bam_files():
    """Find recently created BAM files"""
    bam_files = []
    for path in [Path("data/alignments"), Path("../data/alignments"), Path("../../data/alignments")]:
        if path.exists():
            for bam_file in path.glob("**/*.sorted.bam"):
                bam_files.append((f"{bam_file.name} ({bam_file.parent.name})", str(bam_file)))
    return bam_files

# Callbacks
def on_run_bowtie2_clicked(b):
    with w_bowtie2_output:
        clear_output(wait=True)
        try:
            selected_ref = w_ref_selection.value
            selected_reads = w_reads_selection.value
            
            if not selected_ref or not selected_reads:
                print("Please select both reference and reads")
                return
            
            print(f"Reference: {selected_ref['name']}")
            print(f"Reads: {selected_reads['name']}")
            print(f"Output: {w_outdir.value}")
            
            # Run alignment
            bam_path = run_bowtie2_alignment(
                selected_ref, selected_reads,
                w_outdir.value, w_preset.value,
                w_N.value, w_L.value, w_i.value,
                w_score_min.value, w_threads.value, w_mapq_min.value
            )
            
            print(f"\nAlignment complete! BAM file: {bam_path}")
            
            # Auto-select the new BAM file for analysis
            w_bam_selection.options = [(f"alignment.sorted.bam ({Path(w_outdir.value).name})", bam_path)]
            w_bam_selection.value = bam_path
            
        except Exception as e:
            print(f"Error: {e}")
            import traceback
            traceback.print_exc()

def on_analyze_clicked(b):
    with w_analysis_output:
        clear_output(wait=True)
        try:
            selected_bam = w_bam_selection.value
            if selected_bam:
                quick_bam_analysis(selected_bam)
            else:
                print("Please run Bowtie2 alignment first to create a BAM file")
        except Exception as e:
            print(f"Error: {e}")

def on_refresh_discovery_clicked(b):
    refresh_discovery_and_dropdowns()

# Connect callbacks
w_run_bowtie2.on_click(on_run_bowtie2_clicked)
w_run_analysis.on_click(on_analyze_clicked)
w_refresh_discovery.on_click(on_refresh_discovery_clicked)

# Initialize BAM selection
w_bam_selection = W.Dropdown(
    options=get_recent_bam_files(),
    description="",
    layout=W.Layout(width='600px'),  # Much wider
    style={'description_width': 'initial'}
)

# Display Bowtie2 interface with HTML labels and WIDER LAYOUT
bowtie2_box = W.VBox([
    W.HTML(value="<h3>5) Bowtie2 Alignment</h3>"),
    
    # Discovery refresh button
    W.HBox([
        W.HTML(value="<b>First, refresh discovery to find your library and reads:</b>"),
        w_refresh_discovery
    ]),
    
    # Reference and reads selection - FULL LABELS, WIDER LAYOUT
    W.HBox([
        W.VBox([
            W.HTML(value="<b>Reference Library:</b>"),
            w_ref_selection
        ]),
        W.VBox([
            W.HTML(value="<b>Simulated Reads:</b>"),
            w_reads_selection
        ])
    ]),
    
    # Output directory and preset - FULL LABELS, WIDER LAYOUT
    W.HBox([
        W.VBox([
            W.HTML(value="<b>Output Directory:</b>"),
            w_outdir
        ]),
        W.VBox([
            W.HTML(value="<b>Preset:</b>"),
            w_preset
        ])
    ]),
    
    # First row of parameters - FULL LABELS, WIDER LAYOUT
    W.HBox([
        W.VBox([
            W.HTML(value="<b>N (mismatches):</b>"),
            w_N
        ]),
        W.VBox([
            W.HTML(value="<b>L (seed length):</b>"),
            w_L
        ]),
        W.VBox([
            W.HTML(value="<b>i (seed interval):</b>"),
            w_i
        ])
    ]),
    
    # Second row of parameters - FULL LABELS, WIDER LAYOUT
    W.HBox([
        W.VBox([
            W.HTML(value="<b>Score Min:</b>"),
            w_score_min
        ]),
        W.VBox([
            W.HTML(value="<b>Threads:</b>"),
            w_threads
        ]),
        W.VBox([
            W.HTML(value="<b>Min MAPQ:</b>"),
            w_mapq_min
        ])
    ]),
    
    # Buttons - WIDER LAYOUT
    W.HBox([
        w_run_bowtie2,
        w_run_analysis
    ]),
    
    # BAM file selection - FULL LABEL, WIDER LAYOUT
    W.HBox([
        W.HTML(value="<b>BAM File for Analysis:</b>"),
        w_bam_selection
    ]),
    
    # Output areas
    W.HTML(value="<b>Alignment Output:</b>"),
    w_bowtie2_output,
    W.HTML(value="<b>Analysis Output:</b>"),
    w_analysis_output
])

display(bowtie2_box)

# Initial discovery message
print("Click 'Refresh Discovery' to find your library and reads!")


# =========================
# Observed vs Expected
# =========================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import ipywidgets as W
from IPython.display import display, clear_output, HTML

# ---- UI: controls + outputs ----
w_expect_source = W.Dropdown(
    options=[
        ("Final selection round", "sel_final"),
        ("Selection round (choose)", "sel_round"),
        ("True library: start_frequency", "lib_start"),
        ("True library: uniform", "lib_uniform"),
    ],
    value="sel_final",
    description="Expected from:",
    layout=W.Layout(width='360px'),
    style={'description_width': 'initial'}
)
w_expect_round = W.IntSlider(
    value=3, min=1, max=12, step=1,
    description="Round",
    layout=W.Layout(width='360px'),
    style={'description_width': 'initial'}
)

w_btn_obs_exp = W.Button(
    description="Show Observed vs Expected",
    button_style='info',
    layout=W.Layout(width='260px')
)
out_oxexp = W.Output()

# helper to ensure a BAM is selected
def _require_bam_selected():
    val = globals().get('w_bam_selection', None)
    if val is None or not w_bam_selection.value:
        raise RuntimeError("Select or create a BAM first (run Bowtie2), then choose it in the 'BAM File for Analysis' dropdown.")
    return w_bam_selection.value

# callback for the button
@w_btn_obs_exp.on_click
def _on_show_obs_exp(_):
    out_oxexp.clear_output()
    try:
        try:
            import pysam
        except ImportError:
            with out_oxexp:
                print("Missing dependency: pysam. Install with 'pip install pysam' and re-run.")
            return

        bam_path = _require_bam_selected()
        src = w_expect_source.value

        # ---- Build EXPECTED from session state ----
        if src in ("sel_final", "sel_round"):
            ROUNDS_local = globals().get("ROUNDS", None)
            if not ROUNDS_local:
                with out_oxexp:
                    print("No selection rounds found. Run selection first.")
                return
            if src == "sel_final":
                pool_df = ROUNDS_local[-1][["seq_id", "frequency"]].copy()
            else:
                idx = max(0, min(w_expect_round.value - 1, len(ROUNDS_local) - 1))
                pool_df = ROUNDS_local[idx][["seq_id", "frequency"]].copy()
            pool_df = pool_df.rename(columns={"frequency": "expected"})
            s = pool_df["expected"].sum()
            if s > 0:
                pool_df["expected"] /= s

        elif src == "lib_start":
            LIB_DF_local = globals().get("LIB_DF", None)
            if LIB_DF_local is None or LIB_DF_local.empty:
                with out_oxexp:
                    print("No library found. Generate the library first.")
                return
            pool_df = LIB_DF_local[["seq_id", "start_frequency"]].copy()
            if "start_frequency" not in pool_df or pool_df["start_frequency"].isna().all():
                pool_df["start_frequency"] = 1.0 / len(pool_df)
            s = pool_df["start_frequency"].sum() or 1.0
            pool_df["expected"] = pool_df["start_frequency"] / s
            pool_df = pool_df[["seq_id", "expected"]]

        else:  # lib_uniform
            LIB_DF_local = globals().get("LIB_DF", None)
            if LIB_DF_local is None or LIB_DF_local.empty:
                with out_oxexp:
                    print("No library found. Generate the library first.")
                return
            pool_df = LIB_DF_local[["seq_id"]].copy()
            pool_df["expected"] = 1.0 / len(pool_df)

        # ---- Build OBSERVED from BAM (respect Min MAPQ widget if present) ----
        min_mapq = 0
        if "w_mapq_min" in globals() and w_mapq_min.value is not None:
            try:
                min_mapq = int(w_mapq_min.value)
            except Exception:
                min_mapq = 0

        bam = pysam.AlignmentFile(bam_path, "rb")
        from collections import Counter
        counts = Counter()
        for aln in bam.fetch(until_eof=True):
            if aln.is_unmapped:
                continue
            if aln.mapping_quality is not None and aln.mapping_quality < min_mapq:
                continue
            refname = bam.get_reference_name(aln.reference_id)
            counts[refname] += 1
        bam.close()

        obs = pd.Series(counts, name="reads").rename_axis("seq_id").reset_index()
        total_reads = max(obs["reads"].sum(), 1)
        obs["obs_freq"] = obs["reads"] / total_reads

        merged = pool_df.merge(
            obs[["seq_id", "obs_freq", "reads"]],
            on="seq_id", how="left"
        ).fillna({"obs_freq": 0, "reads": 0})

        # ---- Plot + table ----
        with out_oxexp:
            # scatter plot expected vs observed
            fig, ax = plt.subplots(figsize=(5.5, 5.0))
            ax.scatter(merged["expected"], merged["obs_freq"], s=12, alpha=0.75)
            ax.set_xlabel("Expected frequency")
            ax.set_ylabel("Observed frequency (from alignment)")
            title_src = {
                "sel_final": "Final selection round",
                "sel_round": f"Selection round {w_expect_round.value}",
                "lib_start": "Library start_frequency",
                "lib_uniform": "Library uniform",
            }[src]
            ax.set_title(f"Observed vs expected • {title_src}")

            # R^2 annotation
            x = merged["expected"].to_numpy()
            y = merged["obs_freq"].to_numpy()
            if x.size and y.size:
                xm, ym = x.mean(), y.mean()
                num = ((x - xm) * (y - ym)).sum() ** 2
                den = ((x - xm) ** 2).sum() * ((y - ym) ** 2).sum()
                r2 = float(num / den) if den > 0 else 0.0
                ax.text(0.04, 0.96, f"$R^2$ ≈ {r2:.3f}", transform=ax.transAxes, va="top")
            plt.show()

            # top rows table
            show = (
                merged.assign(diff=(merged["obs_freq"] - merged["expected"]).abs())
                      .sort_values("obs_freq", ascending=False)
                      .head(15)[["seq_id", "expected", "obs_freq", "reads", "diff"]]
            )
            html = (
                show.to_html(index=False)
                    .replace('<table', '<table style="font-size:12px;border-collapse:collapse"')
                    .replace('<th>', '<th style="text-align:left;border-bottom:1px solid #ddd;padding:4px 8px">')
                    .replace('<td>', '<td style="text-align:left;padding:4px 8px">')
            )
            display(HTML(html))

    except Exception as e:
        with out_oxexp:
            import traceback
            traceback.print_exc()
            print(f"\nError: {e}")

# ---- Put the section under your Bowtie2 UI ----
obs_exp_box = W.VBox([
    W.HTML(value="<h3>6) Observed vs expected (from session, no CSV)</h3>"),
    W.HBox([w_expect_source, w_expect_round, w_btn_obs_exp]),
    out_oxexp
])

# If you’ve already displayed bowtie2_box earlier, just display this new box:
display(obs_exp_box)


# ================================
# 7) Noise sweep & recovery visuals
# ================================
import os, json, time, subprocess
from pathlib import Path
from collections import Counter
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import ipywidgets as W
from IPython.display import display, clear_output, HTML

# ---- Helpers: depend on your existing globals / functions ----
def _require_round_pool(round_spec="final"):
    """Return a (seq_id, sequence, frequency) DataFrame from selection rounds or LIB_DF."""
    if round_spec == "library_uniform":
        if 'LIB_DF' not in globals() or LIB_DF is None or LIB_DF.empty:
            raise RuntimeError("No library found. Generate the library first.")
        df = LIB_DF[["seq_id","sequence"]].copy()
        df["frequency"] = 1.0/len(df)
        return df
    if 'ROUNDS' not in globals() or not ROUNDS:
        raise RuntimeError("No selection rounds found. Run selection first.")
    if round_spec == "final":
        return ROUNDS[-1][["seq_id","sequence","frequency"]].copy()
    if isinstance(round_spec, int) and 1 <= round_spec <= len(ROUNDS):
        return ROUNDS[round_spec-1][["seq_id","sequence","frequency"]].copy()
    raise RuntimeError("Invalid round_spec")

def _write_reads_fasta_from_pool(pool_df, reads_total, p_error, seed, outdir):
    """Use your existing ngs_from_pool to simulate reads, write FASTA, return paths."""
    outdir = Path(outdir); outdir.mkdir(parents=True, exist_ok=True)
    reads_df = ngs_from_pool(pool_df[["sequence","frequency"]], reads_total, p_error, seed=seed)
    fasta_path = outdir / "reads.fasta"
    with open(fasta_path, "w") as fh:
        for rid, seq in zip(reads_df["read_id"], reads_df["observed_seq"]):
            fh.write(f">{rid}\n{seq}\n")
    reads_df.to_csv(outdir/"reads.csv", index=False)
    with open(outdir/"params.json","w") as fh:
        json.dump({"reads_total":int(reads_total), "p_error":float(p_error), "seed":int(seed)}, fh, indent=2)
    return str(fasta_path), str(outdir)

def _bowtie2_params_from_widgets():
    """Pull current Bowtie2 params from your alignment widgets."""
    # widgets are expected to exist from your Bowtie2 UI
    preset   = w_preset.value if 'w_preset' in globals() else "--very-sensitive"
    N        = int(w_N.value) if 'w_N' in globals() else 1
    L        = int(w_L.value) if 'w_L' in globals() else 20
    i        = str(w_i.value) if 'w_i' in globals() else "S,1,0.75"
    scoremin = str(w_score_min.value) if 'w_score_min' in globals() else "L,-0.6,-0.6"
    threads  = int(w_threads.value) if 'w_threads' in globals() else 4
    mapq_min = int(w_mapq_min.value) if 'w_mapq_min' in globals() else 0
    return preset, N, L, i, scoremin, threads, mapq_min

def _ensure_index(ref_fasta, outdir):
    """Build Bowtie2 index for a given reference FASTA (multi-FASTA of library)."""
    outdir = Path(outdir)
    outdir.mkdir(parents=True, exist_ok=True)
    index_base = outdir / "reference_index"
    # build only if needed
    needed = [f"{index_base}.{ext}" for ext in ["1.bt2","2.bt2","3.bt2","4.bt2","rev.1.bt2","rev.2.bt2"]]
    if not all(Path(p).exists() for p in needed):
        cmd = ["bowtie2-build","--quiet", ref_fasta, str(index_base)]
        subprocess.run(cmd, check=True, capture_output=True, text=True)
    return str(index_base)

def _write_reference_fasta_from_pool(pool_df, outdir):
    """Write a multi-FASTA of the pool (each seq_id as reference) for alignment."""
    outdir = Path(outdir); outdir.mkdir(parents=True, exist_ok=True)
    ref_fa = outdir / "reference.fa"
    with open(ref_fa, "w") as fh:
        for sid, seq in zip(pool_df["seq_id"], pool_df["sequence"]):
            fh.write(f">{sid}\n{seq}\n")
    return str(ref_fa)

def _align_reads(index_base, reads_fasta, outdir, bowtie_args):
    """Run Bowtie2 + samtools to produce sorted BAM. Return sorted BAM path."""
    outdir = Path(outdir); outdir.mkdir(parents=True, exist_ok=True)
    sam = outdir / "alignment.sam"
    bam = outdir / "alignment.bam"
    sorted_bam = outdir / "alignment.sorted.bam"

    preset, N, L, i, scoremin, threads, _ = bowtie_args
    cmd = [
        "bowtie2",
        preset,
        "-N", str(N),
        "-L", str(L),
        "-i", str(i),
        "--score-min", str(scoremin),
        "-p", str(threads),
        "-f",
        "-x", str(index_base),
        "-U", str(reads_fasta),
        "-S", str(sam)
    ]
    r = subprocess.run(cmd, capture_output=True, text=True)
    if r.returncode != 0:
        raise RuntimeError(f"bowtie2 failed:\n{r.stderr}")

    subprocess.run(["samtools","view","-b",str(sam)], stdout=open(bam,"wb"), check=True)
    subprocess.run(["samtools","sort",str(bam),"-o",str(sorted_bam)], check=True)
    subprocess.run(["samtools","index",str(sorted_bam)], check=True)
    try:
        sam.unlink(); bam.unlink()
    except Exception:
        pass
    return str(sorted_bam)

def _bam_counts_and_scores(bam_path, min_mapq=0):
    """Return (counts_df, scores list) from BAM. Counts by reference (seq_id); AS scores per aligned read."""
    import pysam
    bam = pysam.AlignmentFile(bam_path, "rb")
    counts = Counter()
    scores = []
    for aln in bam.fetch(until_eof=True):
        if aln.is_unmapped: 
            continue
        if aln.mapping_quality is not None and aln.mapping_quality < min_mapq:
            continue
        ref = bam.get_reference_name(aln.reference_id)
        counts[ref] += 1
        # AS tag may be present; guard get_tag
        try:
            scores.append(aln.get_tag("AS"))
        except KeyError:
            pass
    bam.close()
    df = pd.Series(counts, name="reads").rename_axis("seq_id").reset_index()
    total = df["reads"].sum() if len(df) else 0
    df["obs_freq"] = df["reads"] / total if total else 0
    return df, scores

def _topk_overlap(sel_df, bow_df, k=10):
    """Return overlap fraction between top-k by selection frequency vs top-k by observed reads."""
    top_sel = set(sel_df.sort_values("frequency", ascending=False).head(k)["seq_id"])
    top_bow = set(bow_df.sort_values("reads", ascending=False).head(k)["seq_id"])
    return len(top_sel & top_bow) / max(k,1)

# ---- Widgets for sweep ----
w_round_for_sweep = W.Dropdown(
    options=[("Final selection round","final"), ("Choose round…","choose"), ("Library (uniform)","library_uniform")],
    value="final", description="Expected source:", layout=W.Layout(width="280px"),
    style={'description_width':'initial'}
)
w_round_index = W.IntSlider(value=3, min=1, max=12, step=1, description="Round", layout=W.Layout(width="300px"),
                            style={'description_width':'initial'})

w_error_list = W.Text(
    value="0.0001, 0.001, 0.005, 0.01",
    description="Error rates (p):",
    layout=W.Layout(width="360px"),
    style={'description_width':'initial'}
)
w_reads_total = W.IntText(value=10000, description="Reads per rate:", layout=W.Layout(width="220px"),
                          style={'description_width':'initial'})
w_seed_base = W.IntText(value=9000, description="Seed base:", layout=W.Layout(width="200px"),
                        style={'description_width':'initial'})
w_topk = W.IntText(value=10, description="Top-K overlap:", layout=W.Layout(width="200px"),
                   style={'description_width':'initial'})

w_btn_run_sweep = W.Button(description="Run error-rate sweep", button_style="warning", layout=W.Layout(width="220px"))
w_out_sweep = W.Output()

# UI block
sweep_box = W.VBox([
    W.HTML("<h3>7) Noise sweep: AS scores & recovery</h3>"),
    W.HBox([w_round_for_sweep, w_round_index]),
    W.HBox([w_error_list, w_reads_total, w_seed_base, w_topk]),
    w_btn_run_sweep,
    w_out_sweep
])
display(sweep_box)

@w_btn_run_sweep.on_click
def _on_run_sweep(_):
    w_out_sweep.clear_output()
    with w_out_sweep:
        try:
            # pick pool (expected truth) for comparison
            if w_round_for_sweep.value == "final":
                pool = _require_round_pool("final")
                expected_label = "Final round"
            elif w_round_for_sweep.value == "choose":
                pool = _require_round_pool(int(w_round_index.value))
                expected_label = f"Round {int(w_round_index.value)}"
            else:
                pool = _require_round_pool("library_uniform")
                expected_label = "Library uniform"

            # write a reference FASTA for the pool (so seq_id are reference names)
            ref_dir = Path("data/alignments") / f"ref_{int(time.time())}"
            ref_fa = _write_reference_fasta_from_pool(pool, ref_dir)
            index_base = _ensure_index(ref_fa, ref_dir)

            # parse error list
            perrs = [float(p.strip()) for p in w_error_list.value.split(",") if p.strip()]
            reads_total = int(w_reads_total.value)
            seed_base = int(w_seed_base.value)
            topk = int(w_topk.value)
            bow_params = _bowtie2_params_from_widgets()
            min_mapq = bow_params[-1]

            # ground truth top-K (by expected frequency)
            pool_norm = pool.copy()
            s = pool_norm["frequency"].sum() or 1.0
            pool_norm["frequency"] = pool_norm["frequency"] / s

            # collect results
            rows = []
            all_scores = {}
            perr_to_bam = {}

            for j, p in enumerate(perrs):
                run_dir = Path("data/alignments") / f"sweep_p{str(p).replace('.','p')}_{int(time.time())}"
                reads_fa, outdir = _write_reads_fasta_from_pool(pool_norm, reads_total, p, seed_base + j, run_dir)

                # align
                bam_path = _align_reads(index_base, reads_fa, outdir, bow_params)
                perr_to_bam[p] = bam_path

                # analyze
                counts_df, scores = _bam_counts_and_scores(bam_path, min_mapq=min_mapq)
                all_scores[p] = scores

                # overlap
                overlap = _topk_overlap(pool_norm, counts_df, k=topk)

                # R^2 on frequencies
                merged = pool_norm[["seq_id","frequency"]].merge(
                    counts_df[["seq_id","obs_freq","reads"]],
                    on="seq_id", how="left"
                ).fillna({"obs_freq":0,"reads":0})
                x = merged["frequency"].to_numpy()
                y = merged["obs_freq"].to_numpy()
                if x.size and y.size:
                    xm, ym = x.mean(), y.mean()
                    num = ((x-xm)*(y-ym)).sum()**2
                    den = ((x-xm)**2).sum() * ((y-ym)**2).sum()
                    r2 = float(num/den) if den>0 else 0.0
                else:
                    r2 = 0.0

                rows.append({
                    "p_error": p,
                    "reads_total": reads_total,
                    "R2_freq": r2,
                    f"top{topk}_overlap": overlap,
                    "aligned_reads": int(counts_df["reads"].sum())
                })

            summary = pd.DataFrame(rows).sort_values("p_error")
            display(HTML("<b>Summary across error rates</b>"))
            display(summary)

            # --- Plots ---
            # 1) Recovery vs error (top-K overlap)
            fig1, ax1 = plt.subplots(figsize=(6,3.5))
            ax1.plot(summary["p_error"], summary[f"top{topk}_overlap"], marker="o")
            ax1.set_xlabel("Per-base error rate (p)")
            ax1.set_ylabel(f"Top-{topk} overlap")
            ax1.set_title(f"Recovery vs noise • expected: {expected_label}")
            ax1.set_ylim(0,1.02)
            plt.show()

            # 2) R^2 vs error
            fig2, ax2 = plt.subplots(figsize=(6,3.5))
            ax2.plot(summary["p_error"], summary["R2_freq"], marker="o")
            ax2.set_xlabel("Per-base error rate (p)")
            ax2.set_ylabel(r"$R^2$ (obs vs expected freq)")
            ax2.set_title("Agreement vs noise")
            ax2.set_ylim(0,1.02)
            plt.show()

            # 3) AS score distributions per error (small multiples)
            ncols = min(3, len(perrs))
            nrows = int(np.ceil(len(perrs)/ncols))
            fig3, axes = plt.subplots(nrows, ncols, figsize=(5*ncols, 3*nrows), squeeze=False)
            for idx, p in enumerate(perrs):
                r, c = divmod(idx, ncols)
                ax = axes[r][c]
                scores = all_scores.get(p, [])
                if scores:
                    ax.hist(scores, bins=40)
                ax.set_title(f"AS scores @ p={p}")
                ax.set_xlabel("AS"); ax.set_ylabel("Count")
            # remove empty axes
            for k in range(len(perrs), nrows*ncols):
                r, c = divmod(k, ncols)
                fig3.delaxes(axes[r][c])
            fig3.suptitle("Alignment score (AS) distributions vs noise", y=1.02)
            plt.tight_layout()
            plt.show()

            # 4) Quick Top-10 comparison at the *noisiest* rate
            worst_p = summary.sort_values("p_error").iloc[-1]["p_error"]
            worst_bam = perr_to_bam[worst_p]
            counts_df, _ = _bam_counts_and_scores(worst_bam, min_mapq=min_mapq)
            top_sel = (pool_norm.sort_values("frequency", ascending=False)
                                 .head(10)[["seq_id","frequency"]]
                                 .rename(columns={"frequency":"sel_frequency"}))
            top_bow = (counts_df.sort_values("reads", ascending=False)
                                 .head(10)[["seq_id","reads","obs_freq"]]
                                 .rename(columns={"reads":"bowtie_reads","obs_freq":"bow_freq"}))
            comp = top_sel.merge(top_bow, on="seq_id", how="outer").fillna(0)
            comp["sel_rank"] = comp["sel_frequency"].rank(ascending=False, method="min")
            comp["bow_rank"] = comp["bowtie_reads"].rank(ascending=False, method="min")
            display(HTML(f"<b>Top-10: selection vs Bowtie2 @ p={worst_p}</b>"))
            display(comp.sort_values(["bowtie_reads","sel_frequency"], ascending=[False,False]))

        except Exception as e:
            import traceback; traceback.print_exc()
            print(f"\nError: {e}")





# === Step 8 — NGS visuals===
import os, re, glob, math, io, itertools
import numpy as np, pandas as pd
import matplotlib.pyplot as plt
from collections import Counter, defaultdict
from pathlib import Path
from IPython.display import display, Markdown, HTML

# -----------------------
# 0) Tiny utilities
# -----------------------
def _safe(v): return v in globals()

AA20 = list("ACDEFGHIKLMNPQRSTVWY")
AA_SET = set(AA20)
DNA = set("ACGT")

# quick DNA->AA (frame 0 only, deterministic)
_tbl = {
 'TTT':'F','TTC':'F','TTA':'L','TTG':'L','CTT':'L','CTC':'L','CTA':'L','CTG':'L',
 'ATT':'I','ATC':'I','ATA':'I','ATG':'M','GTT':'V','GTC':'V','GTA':'V','GTG':'V',
 'TCT':'S','TCC':'S','TCA':'S','TCG':'S','CCT':'P','CCC':'P','CCA':'P','CCG':'P',
 'ACT':'T','ACC':'T','ACA':'T','ACG':'T','GCT':'A','GCC':'A','GCA':'A','GCG':'A',
 'TAT':'Y','TAC':'Y','TAA':'*','TAG':'*','CAT':'H','CAC':'H','CAA':'Q','CAG':'Q',
 'AAT':'N','AAC':'N','AAA':'K','AAG':'K','GAT':'D','GAC':'D','GAA':'E','GAG':'E',
 'TGT':'C','TGC':'C','TGA':'*','TGG':'W','CGT':'R','CGC':'R','CGA':'R','CGG':'R',
 'AGT':'S','AGC':'S','AGA':'R','AGG':'R','GGT':'G','GGC':'G','GGA':'G','GGG':'G'
}
def _dna_to_aa(s):
    s = s.upper()
    n = len(s)//3
    return "".join(_tbl.get(s[3*i:3*i+3], 'X') for i in range(n))

def _infer_var_positions(aa_list, max_positions=80, min_unique=2):
    if not aa_list: return []
    L = min(len(x) for x in aa_list)
    pos = []
    for i in range(L):
        uniq = {x[i] for x in aa_list if len(x)>i}
        if len(uniq) >= min_unique:
            pos.append(i)
    return pos[:max_positions]

def _entropy(fracs):
    x = np.array([v for v in fracs.values() if v>0], float)
    return float(-(x*np.log2(x)).sum()) if x.size else 0.0

def _bars_from_comp(comp, pos_list):
    # comp: {pos -> Counter(fraction)}
    return np.array([[comp[i].get(aa,0.0) for i in pos_list] for aa in AA20])

def _brief(x, n=6):
    x = list(x)
    return ", ".join(map(str, x[:n])) + ("…" if len(x) > n else "")

def _load_fasta(path):
    names, seqs = [], []
    with open(path) as f:
        name, buf = None, []
        for line in f:
            if line.startswith(">"):
                if name is not None:
                    seqs.append("".join(buf).replace(" ", "").replace("\r","").replace("\n",""))
                name = line[1:].strip().split()[0]; names.append(name); buf = []
            else:
                buf.append(line.strip())
        if name is not None:
            seqs.append("".join(buf).replace(" ", ""))
    return names, seqs

def _normalize_id(s: str) -> str:
    s0 = s.strip().split()[0].replace("|","_")
    s0 = re.sub(r"^(ref|gi|id|seq|var|v)_?", "", s0, flags=re.IGNORECASE)
    m = re.search(r"(\d+)$", s0)
    if m: return str(int(m.group(1)))
    return s0

def _guess_ref_fasta_paths():
    cands = []
    for key in ["ref_fa","REF_FASTA","REFERENCE_FASTA","reference_fasta","REF_FA"]:
        if key in globals():
            p = str(globals()[key])
            if p and os.path.exists(p): cands.append(p)
    cands += glob.glob("**/reference.fa", recursive=True)
    cands += glob.glob("**/ref.fa", recursive=True)
    cands += glob.glob("**/selection/ref.fa", recursive=True)
    return [p for p in cands if os.path.isfile(p)]

# -----------------------
# 1) Inputs from session
# -----------------------
if not _safe('LIB_DF') or LIB_DF is None or LIB_DF.empty:
    raise RuntimeError("LIB_DF not found/empty. Generate the library earlier.")

if not _safe('ROUNDS') or not ROUNDS:
    raise RuntimeError("ROUNDS not found/empty. Run selection rounds first.")

# Choose pools to compare (paper did 'Round0' vs 'Round2'; here: LIB vs FINAL)
POOL_A = ("Library", LIB_DF[["seq_id","sequence"]].assign(frequency=lambda d: 1.0/len(d)))
POOL_B = ("Final",   ROUNDS[-1][["seq_id","sequence","frequency"]].copy())

# AA strings for var-pos inference (from library)
lib_AA = [
    seq if (set(seq) <= AA_SET) else _dna_to_aa(seq)
    for seq in POOL_A[1]["sequence"].tolist()
]
if not _safe('VAR_POS') or VAR_POS is None:
    VAR_POS = _infer_var_positions(lib_AA)

# -----------------------
# 2) OBSERVED counts + robust name→AA mapping (fix for empty observed plot)
# -----------------------
# 2a) Pick BAM/SAM
bam_path = None
if 'w_bam_selection' in globals() and getattr(w_bam_selection, "value", ""):
    bam_path = w_bam_selection.value
elif 'sorted_bam' in globals():
    bam_path = str(sorted_bam)
elif 'sam' in globals():
    try:
        if sam and hasattr(sam, "exists") and sam.exists():
            bam_path = str(sam)
    except Exception:
        pass
if not bam_path or not os.path.exists(bam_path):
    raise RuntimeError("No BAM/SAM found. Select one in the UI or set `bam_path`.")

# 2b) Read observed counts
counts = Counter()
try:
    import pysam
    use_bam = bam_path.lower().endswith((".bam",".cram"))
    af = pysam.AlignmentFile(bam_path, "rb" if use_bam else "r")
    min_mapq = int(w_mapq_min.value) if 'w_mapq_min' in globals() else 0
    for aln in af.fetch(until_eof=True):
        if aln.is_unmapped: continue
        if aln.mapping_quality is not None and aln.mapping_quality < min_mapq: continue
        rn = af.get_reference_name(aln.reference_id)
        counts[rn] += 1
    af.close()
except Exception:
    if bam_path.lower().endswith(".sam"):
        with open(bam_path) as fh:
            for line in fh:
                if line.startswith("@"): continue
                p = line.rstrip("\n").split("\t")
                if len(p) >= 3 and p[2] != "*" and (int(p[1]) & 0x4) == 0:
                    counts[p[2]] += 1
    else:
        raise

obs_df = pd.Series(counts, name="reads").rename_axis("seq_id").reset_index()
tot_reads = obs_df["reads"].sum() if len(obs_df) else 0
obs_df["obs_freq"] = obs_df["reads"]/tot_reads if tot_reads else 0.0
refnames = obs_df["seq_id"].tolist()

display(Markdown(
    f"**Observed summary**  \n"
    f"- Aligned reads counted: **{tot_reads:,}**  \n"
    f"- Unique references with ≥1 read: **{len(refnames):,}**  \n"
    f"- Example reference IDs: `{_brief(refnames)}`"
))

# 2c) Diversified positions (local copy to avoid side-effects)
if 'VAR_POS' in globals() and VAR_POS:
    VAR_POS_LOCAL = list(VAR_POS)
else:
    # infer from library AA
    VAR_POS_LOCAL = _infer_var_positions(lib_AA)
display(Markdown(f"- Positions plotted: **{len(VAR_POS_LOCAL)}**"))

# 2d) Build name→AA mapping (tables → normalized ids → reference FASTA)
name2aa_obs = {}

tables = []
if 'ROUNDS' in globals() and ROUNDS:
    tables.append(ROUNDS[-1][["seq_id","sequence"]])
if 'LIB_DF' in globals() and not LIB_DF.empty and "sequence" in LIB_DF.columns:
    tables.append(LIB_DF[["seq_id","sequence"]])

for t in tables:
    for sid, seq in zip(t["seq_id"], t["sequence"]):
        name2aa_obs[str(sid)] = seq

# normalize ids (e.g., A_000123 -> 123)
direct_hits = sum(1 for r in refnames if r in name2aa_obs)
if direct_hits < len(refnames):
    norm_key_to_seq = {}
    for t in tables:
        for sid, seq in zip(t["seq_id"], t["sequence"]):
            norm_key_to_seq[_normalize_id(str(sid))] = seq
    for rn in refnames:
        if rn in name2aa_obs: continue
        k = _normalize_id(rn)
        if k in norm_key_to_seq:
            name2aa_obs[rn] = norm_key_to_seq[k]

# last resort: parse the FASTA used for the index
remaining = [r for r in refnames if r not in name2aa_obs]
fa_used = None
if remaining:
    for fa in _guess_ref_fasta_paths():
        try:
            names, seqs = _load_fasta(fa)
            if not names: continue
            fa_map = dict(zip(names, seqs))
            hits = 0
            for rn in list(remaining):
                s = fa_map.get(rn)
                if not s: continue
                aa = _dna_to_aa(s) if set(s.upper()) <= DNA else s.strip().upper()
                name2aa_obs[rn] = aa
                hits += 1
            if hits:
                fa_used = fa
                break
        except Exception:
            pass

missing = [r for r in refnames if r not in name2aa_obs]
mapped = len(refnames) - len(missing)
note_lines = [f"- References mapped to AA sequences: **{mapped:,}/{len(refnames):,}**"]
if fa_used: note_lines.append(f"- Mapped via reference FASTA: `{fa_used}`")
if missing:
    note_lines.append(f":warning: Unmapped reference IDs (few): `{_brief(missing)}`")
    note_lines.append("  • Ensure the Bowtie reference FASTA used **seq_id** headers, or set `ref_fa` so this cell can translate DNA → AA.")
display(Markdown("\n".join(note_lines)))

# -----------------------
# 3) (A) “Round curve”: cumulative share across top-N
# -----------------------
def _round_curve_from_counts(counts_series, N=100):
    c = np.sort(np.array(counts_series, dtype=float))[::-1]
    if c.size == 0:
        return np.array([0]), np.array([0])
    N = min(N, c.size)
    c = c[:N]
    cum_share = np.cumsum(c)/c.sum()
    return np.arange(1, N+1), cum_share

figA, axA = plt.subplots(1,1, figsize=(6,3.6))
xA, yA = _round_curve_from_counts(obs_df["reads"])
axA.plot(xA, yA, marker='o', ms=3, lw=1.5)
axA.set_xlabel("Rank (top-N variants)")
axA.set_ylabel("Cumulative fraction of reads")
axA.set_title("Cumulative reads across top variants")
axA.set_ylim(0, 1.02)
plt.show()

# -----------------------
# 4) (B) Per-position AA composition for two pools
# -----------------------
def _position_composition(pool_df, var_pos):
    comp = {i: Counter() for i in var_pos}
    if "frequency" in pool_df.columns:
        w = dict(zip(pool_df["seq_id"], pool_df["frequency"]))
        w_norm = sum(w.values()) or 1.0
        w = {k:v/w_norm for k,v in w.items()}
        for sid, seq in zip(pool_df["seq_id"], pool_df["sequence"]):
            aa = seq if (set(seq) <= AA_SET) else _dna_to_aa(seq)
            wt = w.get(sid, 0)
            for i in var_pos:
                if i < len(aa): comp[i][aa[i]] += wt
    else:
        for seq in pool_df["sequence"]:
            aa = seq if (set(seq) <= AA_SET) else _dna_to_aa(seq)
            for i in var_pos:
                if i < len(aa): comp[i][aa[i]] += 1
    for i in comp:
        tot = sum(comp[i].values())
        if tot == 0: continue
        for a in list(comp[i].keys()):
            comp[i][a] /= tot
    return comp

comp_A = _position_composition(POOL_A[1], VAR_POS_LOCAL)

# observed (read-weighted) composition using the robust name→AA mapping
comp_B = {i: Counter() for i in VAR_POS_LOCAL}
observed_variants_used = 0
for sid, reads in zip(obs_df["seq_id"], obs_df["reads"]):
    aa = name2aa_obs.get(sid)
    if not aa: continue
    observed_variants_used += 1
    for i in VAR_POS_LOCAL:
        if i < len(aa):
            comp_B[i][aa[i]] += reads
for i in comp_B:
    tot = sum(comp_B[i].values())
    if tot == 0: continue
    for a in list(comp_B[i].keys()):
        comp_B[i][a] /= tot

display(Markdown(f"- Variants contributing to observed composition: **{observed_variants_used:,}**"))

def _plot_comp(comp, title):
    pos = VAR_POS_LOCAL
    if not pos:
        display(Markdown("> No diversified positions inferred/found.")); return
    mat = _bars_from_comp(comp, pos)
    fig, ax = plt.subplots(1,1, figsize=(min(12, 0.55*len(pos)+2),3.6))
    bottom = np.zeros(len(pos))
    for k, aa in enumerate(AA20):
        ax.bar(range(len(pos)), mat[k], bottom=bottom, width=0.86, label=aa)
        bottom += mat[k]
    ax.set_xticks(range(len(pos))); ax.set_xticklabels([str(p+1) for p in pos], rotation=0)
    ax.set_ylim(0,1.0)
    ax.set_xlabel("Diversified position (AA index, 1-based)")
    ax.set_ylabel("Fraction")
    ax.set_title(title)
    handles, labels = ax.get_legend_handles_labels()
    ax.legend(handles[:20], labels[:20], ncol=10, fontsize=7, loc='upper center', bbox_to_anchor=(0.5,-0.18))
    plt.tight_layout(); plt.show()

_plot_comp(comp_A, f"Expected AA composition per diversified position • {POOL_A[0]}")
_plot_comp(comp_B, f"Observed (read-weighted) AA composition per diversified position • {POOL_B[0]}")

if observed_variants_used == 0:
    display(Markdown(
        ":warning: Observed plot still empty (no BAM refs mapped to sequences). "
        "Rebuild your reference FASTA with true `seq_id` headers or set `ref_fa` so this cell can translate DNA→AA."
    ))

# -----------------------
# 5) (C) Enrichment heatmap: log2(Observed/Expected)
# -----------------------
pos = VAR_POS_LOCAL
M = np.zeros((len(AA20), len(pos)))
for j,i in enumerate(pos):
    for k,aa in enumerate(AA20):
        d = comp_A[i].get(aa, 0.0)
        s = comp_B[i].get(aa, 0.0)
        M[k,j] = math.log2((s + 1e-6) / (d + 1e-6))

fig, ax = plt.subplots(1,1, figsize=(min(12, 0.55*len(pos)+2), 5))
vmax = np.percentile(np.abs(M), 95) if M.size else 1.0
im = ax.imshow(M, cmap="coolwarm", vmin=-vmax, vmax=vmax, aspect='auto', interpolation='nearest')
ax.set_yticks(range(len(AA20))); ax.set_yticklabels(AA20)
ax.set_xticks(range(len(pos)));  ax.set_xticklabels([str(p+1) for p in pos])
ax.set_xlabel("Position (AA index, 1-based)"); ax.set_ylabel("AA")
ax.set_title("log2 Enrichment ratio (Observed / Expected)")
cbar = plt.colorbar(im, ax=ax, shrink=0.9); cbar.set_label("log2(ER)")
plt.tight_layout(); plt.show()

# -----------------------
# 6) (D) Sequence-space scatter via t-SNE of k-mer embeddings
# -----------------------
def _kmer_counts(seqs, k=3):
    K = [''.join(p) for p in itertools.product(AA20, repeat=k)]
    idx = {kmer:i for i,kmer in enumerate(K)}
    X = np.zeros((len(seqs), len(K)), dtype=float)
    for r, s in enumerate(seqs):
        aa = s if (set(s) <= AA_SET) else _dna_to_aa(s)
        for i in range(len(aa)-k+1):
            kmer = aa[i:i+k]
            if kmer in idx:
                X[r, idx[kmer]] += 1
        if X[r].sum() > 0:
            X[r] /= X[r].sum()
    return X

try:
    from sklearn.manifold import TSNE
    Aseq = POOL_A[1]["sequence"].sample(min(1500, len(POOL_A[1])), random_state=7).tolist()
    Bseq = POOL_B[1]["sequence"].sample(min(1500, len(POOL_B[1])), random_state=7).tolist()
    XA = _kmer_counts(Aseq, k=3); XB = _kmer_counts(Bseq, k=3)
    XY = np.vstack([XA, XB])
    ts = TSNE(n_components=2, perplexity=30, learning_rate='auto', init='pca', random_state=7)
    Z = ts.fit_transform(XY)
    fig, ax = plt.subplots(1,1, figsize=(5.2,4.6))
    nA = XA.shape[0]
    ax.scatter(Z[:nA,0], Z[:nA,1], s=6, alpha=0.6, label=POOL_A[0], c="#2563eb")
    ax.scatter(Z[nA:,0], Z[nA:,1], s=6, alpha=0.6, label=POOL_B[0], c="#10b981")
    ax.set_title("t-SNE of k-mer embeddings"); ax.set_xlabel("tSNE_1"); ax.set_ylabel("tSNE_2")
    ax.legend(); plt.show()
except Exception:
    display(Markdown("> t-SNE skipped (scikit-learn not available or small pool)."))

# -----------------------
# 7) (F) Cumulative unique variants vs minimum read threshold
# -----------------------
c = np.sort(obs_df["reads"].to_numpy())[::-1]
if c.size:
    thr = np.unique(np.clip(c, 1, c.max()))
    uniq = np.array([(c >= t).sum() for t in thr], dtype=int)
    fig, ax = plt.subplots(1,1, figsize=(6,3.6))
    ax.plot(thr, uniq, marker='o', lw=1.5, c="#0ea5e9")
    ax.set_xscale("log")
    ax.set_xlabel("Minimum read count (log scale)")
    ax.set_ylabel("Unique variants ≥ threshold")
    ax.set_title("Cumulative unique vs read threshold"); plt.show()
else:
    display(Markdown("> No aligned reads available to compute unique-vs-threshold."))

# -----------------------
# 8) Short, value-dependent interpretation
# -----------------------
def _pct(x): return f"{100*x:.1f}%"
top20_share = 0.0
if c.size:
    top20 = min(20, c.size)
    top20_share = float(c[:top20].sum()/c.sum())
medent_A = np.median([_entropy(comp_A[i]) for i in VAR_POS_LOCAL]) if VAR_POS_LOCAL else 0.0
medent_B = np.median([_entropy(comp_B[i]) for i in VAR_POS_LOCAL]) if VAR_POS_LOCAL else 0.0
delta_ent = medent_B - medent_A

reg_lines = []
for label, slc in [("CDR1-like", globals().get("CDR1_SLICE", None)),
                   ("CDR3-like", globals().get("CDR3_SLICE", None))]:
    if slc is None: continue
    pos_in = [p for p in VAR_POS_LOCAL if (p >= (slc.start or 0)) and (p < (slc.stop or 10**9))]
    vals = []
    for i in pos_in:
        for aa in AA20:
            d = comp_A[i].get(aa,0.0); s = comp_B[i].get(aa,0.0)
            vals.append(math.log2((s+1e-6)/(d+1e-6)))
    if vals:
        m = float(np.mean(vals))
        trend = "enriched" if m>0 else ("depleted" if m<0 else "unchanged")
        reg_lines.append(f"{label}: mean log₂ER ≈ {m:+.2f} ({trend})")

display(Markdown(
f"""
**Step 8 quick readout**

- Top-20 variants capture **~{_pct(top20_share)}** of aligned reads (steeper ‘round curve’ ⇒ stronger winner-take-most behavior).  
- Median per-position entropy changed by **{delta_ent:+.2f} bits** (negative ⇒ focusing on tolerated residues).  
- Regional enrichment: {("; ".join(reg_lines) if reg_lines else "no region slices provided")}.  

**Why some panels can look “strong” while others look “moderate”**  
Top-N dominance can be very high **and** inequality across *all* variants still be moderate if the mid-rank tail keeps non-trivial coverage. A few variants win big, but many others still receive reads, which spreads area under the Lorenz-style curve and lowers global inequality measures.
"""
))



